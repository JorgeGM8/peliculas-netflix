{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwI4hrevoNAg"
      },
      "source": [
        "# Películas de Netflix - Análisis con pyspark\n",
        "\n",
        "Jorge Galeano Maté"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qC8knv0oNAj"
      },
      "source": [
        "### Instrucciones\n",
        "\n",
        "1. Leer todos los csv descomprimidos guardados en la ruta de vuestro tmp en una sola línea de código (pista, usar wildcards para leer más de un fichero a la vez).\n",
        "2. Analiza las columnas y renómbralos con un nombre que tenga sentido para cada una.\n",
        "3. Limpia el dataframe para que no existan nulos, adicionalmente elimina todos los valores que no se correspondan con el resto de datos de la columna.\n",
        "4. Revisa el tipo de dato de cada columna y parséalo según corresponda (la columna duración debe ser numérica).\n",
        "5. Calcula la duración media en función del país.\n",
        "6. Filtra las películas que contengan la palabra _music_ en su descripción y que su duración sea mayor a 90 minutos, ¿cuál es el actor que más películas ha realizado bajo estas condiciones?\n",
        "7. Para el actor que más producciones ha realizado calcula cuántas semanas han pasado desde su primera producción hasta su última.\n",
        "8. Transforma la columna de géneros para que su contenido sea un array con los valores de cada género por registro.\n",
        "9. ¿Cuántas producciones se han realizado en un único país y cuántas tienen 2 o más países?\n",
        "10. Escribe el dataframe final como un fichero parquet.\n",
        "\n",
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYCKWGseoeao"
      },
      "source": [
        "Antes de comenzar, descargamos los datos según las instrucciones y descomprimimos los csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3YwEi5qokA9",
        "outputId": "ae53ba40-3466-492e-bd12-e748f6d82414"
      },
      "outputs": [],
      "source": [
        "!wget -O /tmp/netflix_titles_dirty_01.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_01.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_02.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_02.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_03.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_03.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_04.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_04.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_05.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_05.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_06.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_06.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_07.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_07.csv.gz?raw=True'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgRTXsxapK8V",
        "outputId": "bfaa8a88-8241-4f3d-d230-738c8c4de723"
      },
      "outputs": [],
      "source": [
        "!ls /tmp/netflix_titles* # Comprobación de descarga correcta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tJWYzsYpbwm"
      },
      "outputs": [],
      "source": [
        "# Descomprimimos los archivos e indicamos que se guarden los datos en un csv.\n",
        "!gunzip -c /tmp/netflix_titles_dirty_01.csv.gz > /tmp/netflix_titles_dirty_01.csv\n",
        "!gunzip -c /tmp/netflix_titles_dirty_02.csv.gz > /tmp/netflix_titles_dirty_02.csv\n",
        "!gunzip -c /tmp/netflix_titles_dirty_03.csv.gz > /tmp/netflix_titles_dirty_03.csv\n",
        "!gunzip -c /tmp/netflix_titles_dirty_04.csv.gz > /tmp/netflix_titles_dirty_04.csv\n",
        "!gunzip -c /tmp/netflix_titles_dirty_05.csv.gz > /tmp/netflix_titles_dirty_05.csv\n",
        "!gunzip -c /tmp/netflix_titles_dirty_06.csv.gz > /tmp/netflix_titles_dirty_06.csv\n",
        "!gunzip -c /tmp/netflix_titles_dirty_07.csv.gz > /tmp/netflix_titles_dirty_07.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbXP6L2qoNAk"
      },
      "source": [
        "### 1. Leer todos los csv descomprimidos guardados en la ruta de vuestro tmp en una sola línea de código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTr2U04sEWY"
      },
      "source": [
        "Creo la sesión de Spark y leo todos los csv a la vez aprovechando el uso de wildcards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTrGEp2uoZHM",
        "outputId": "a5972250-361a-4325-a052-875d2562eed8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"trabajo\").getOrCreate()\n",
        "\n",
        "# Para leer todos los csv de una vez, uso wildcards (*) para sustituir los números y así seleccionar todos.\n",
        "df = spark.read.csv('/tmp/netflix_titles_dirty_0*.csv', sep='\\t', inferSchema=True)\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-PmHi4Mvd9r"
      },
      "source": [
        "### 2. Analiza las columnas y renómbralos con un nombre que tenga sentido para cada una."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Jm8vcQvl12"
      },
      "source": [
        "Para ello, utilizo la función `withColumnsRenamed()`, que permite editar varias columnas a la vez.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOrzYyBmv7OA",
        "outputId": "d6504c4b-74c8-4dbd-ceb3-50c872773950"
      },
      "outputs": [],
      "source": [
        "df = df.withColumnsRenamed({\n",
        "    '_c0' : 'id',\n",
        "    '_c1' : 'tipo',\n",
        "    '_c2' : 'titulo',\n",
        "    '_c3' : 'direccion',\n",
        "    '_c4' : 'elenco',\n",
        "    '_c5' : 'pais',\n",
        "    '_c6' : 'fecha_netflix', # Es la fecha en la que Netflix publicó la serie o película en su plataforma.\n",
        "    '_c7' : 'estreno', # Es la fecha en la que se estrenó la serie o película.\n",
        "    '_c8' : 'clasificacion',\n",
        "    '_c9' : 'duracion',\n",
        "    '_c10' : 'genero',\n",
        "    '_c11' : 'sinopsis'\n",
        "})\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5mIvIMA1Ax2"
      },
      "source": [
        "### 3. Limpia el dataframe para que no existan nulos, adicionalmente elimina todos los valores que no se correspondan con el resto de datos de la columna. _Eliminar las filas que sean de series para trabajar solo con películas._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwsg4SchTVxv",
        "outputId": "7df96d29-2869-4fb2-fbf7-fdf20aabcb31"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, isnan, count, col\n",
        "\n",
        "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show() # Muestro el número de nulos en cada columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvkVIW0S1G0b",
        "outputId": "a7d400d6-e020-478e-85be-d9f29da92fe2"
      },
      "outputs": [],
      "source": [
        "df = df.dropna() # Con esta función elimino todas las filas que contengan nulos.\n",
        "\n",
        "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show() # Muestro de nuevo el número de nulos en cada columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HV_s8J82TKO",
        "outputId": "b0ffa70c-50c6-4c1a-8c00-7d82f2af1d23"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = (\n",
        "    df.filter(col('id').rlike('^\\\\d{8}$')) # Uso rlike() para definir la expresión regular y eliminar todos los id que no sean exactamente 8 números.\n",
        "    .filter('tipo == \"Movie\"') # Filtro para dejar solo tipo Movie.\n",
        ")\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFl1S82KB_Ri"
      },
      "source": [
        "### 4. Revisa el tipo de dato de cada columna y parséalo según corresponda (la columna duración debe ser numérica)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI2cMzbXCD8y"
      },
      "source": [
        "Para comprobar el esquema del dataframe, se puede usar el comando `printSchema()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqiiFgFbCDnJ",
        "outputId": "874e1b85-e205-4f7f-d9dd-0f5f4eeffa1e"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s06O_8flCOnS"
      },
      "source": [
        "Esto nos muestra que todos están parseados como _string_. Cambio las siguientes columnas:\n",
        "\n",
        "- __id:__ int. _Porque solo son números enteros._\n",
        "- __direccion:__ string array. _Porque puede contener uno o varios directores._\n",
        "- __elenco:__ string array. _Porque puede contener uno o varios actores._\n",
        "- __pais:__ string array. _Porque puede contener uno o varios países._\n",
        "- __fecha_netflix:__ fecha. _Porque es una fecha._\n",
        "- __estreno:__ int. _Porque es solo un año (un número entero)._\n",
        "- __duracion:__ int. _Porque son solo números enteros._\n",
        "- __genero:__ string array. _Porque puede contener uno o varios géneros._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H2iGxky7Xhl"
      },
      "source": [
        "Antes de continuar, la columna de \"fecha_netflix\" tiene un formato especial y variado. Tenemos algunos espacios antes o después del texto, los cuales hay que eliminar; números de día que a veces tienen solo uno y a veces dos; y en formato con el mes completo, espacios y comas.\n",
        "\n",
        "Para gestionarlo, elimino los espacios con `trim()` (esto lo hago con todas las columnas, ya de paso) y decido separar la cadena en 3 columnas, para mes, día y año. Luego, elimino la coma que hay en el día y obligo a que sean dos números en vez de uno, añadiendo un 0 si solo hubiese uno. Por último, vuelvo a unir los tres en la columna original para que tenga todo el mismo formato y poder parsearlo a fecha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muHMfn68d12c"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import trim, split, regexp_replace, lpad, concat, lit, StringType\n",
        "\n",
        "df = df.select([trim(col(x)).alias(x) for x in df.columns]) # Elimino espacios al principio y al final en todas las columnas.\n",
        "\n",
        "df = (\n",
        "    df.withColumn('mes', split(col('fecha_netflix'), ' ').getItem(0)) # Obtengo el mes.\n",
        "    .withColumn('dia', split(col('fecha_netflix'), ' ').getItem(1)) # Obtengo el día.\n",
        "    .withColumn('año', split(col('fecha_netflix'), ' ').getItem(2)) # Obtengo el año.\n",
        ")\n",
        "\n",
        "df = df.withColumn('dia', regexp_replace(col('dia'), ',', '')) # Elimino la coma de día.\n",
        "\n",
        "df = df.withColumn('dia', lpad(col('dia'), 2, '0')) # Obligo a que haya 2 caracteres, y que si no los hay, añada un 0 a la izquierda.\n",
        "\n",
        "df = (\n",
        "    df.withColumn('fecha_netflix', concat(col('mes'), lit(' '), col('dia'), lit(', '), col('año'))) # Uno los datos de nuevo.\n",
        "    .drop('mes', 'dia', 'año') # Elimino las columnas que había creado temporalmente.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gksVVh8h_eFO"
      },
      "source": [
        "Una vez hecho esto, procedo a cambiar el tipo de dato de todas las columnas al correspondiente, usando `withColumns()`. También elimino la parte de \"min\" de la columna de duración, para ello lo hago fuera individualmente con `withColumn()` porque primero necesito que se elimine la parte de texto para poder pasarlo correctamente a int."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d3DdzBkDowQ",
        "outputId": "3ab6eab6-a5ca-4046-c9c6-c113e244cd7c"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "df = (\n",
        "    df.withColumns({\n",
        "        'id' : col('id').cast('int'),\n",
        "        'direccion' : split(df['direccion'], ', '),\n",
        "        'elenco' : split(df['elenco'], ', '),\n",
        "        'pais' : split(df['pais'], ', '),\n",
        "        'fecha_netflix' : to_date(col('fecha_netflix'), 'MMMM dd, yyyy'),\n",
        "        'estreno' : col('estreno').cast('int'),\n",
        "        'duracion' : regexp_replace(col('duracion'), ' min', ''),\n",
        "        'genero' : split(df['genero'], ', ')\n",
        "    })\n",
        "    .withColumn('duracion', col('duracion').cast('int'))\n",
        ")\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNtjjzCG_otA",
        "outputId": "1339b314-db6e-45e6-b385-86c3fece93c1"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2XKdRdTAtDZ"
      },
      "source": [
        "### 5. Calcula la duración media en función del país."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2UKX6EqBDYO"
      },
      "source": [
        "Para hacer esto, es necesario agrupar por país, usando la función `groupBy()`. No obstante, la columna es un array de strings, ya que a veces hay varios países. Por tanto, lo que hago será crear una fila con los mismos datos pero para cada país, es decir, si tengo Estados Unidos y Canadá en una película, creará dos filas con los mismos datos, pero una con Estados Unidos y otra con Canadá. Para ello uso la función `explode()`.\n",
        "\n",
        "A partir de ahí, aplico la media con `mean()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTVG0dAtAsuQ",
        "outputId": "e23b1577-e725-4560-a4c5-f8e12a188be4"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import explode, mean\n",
        "\n",
        "df_paises = df.withColumn('paises_exp', explode(col('pais')))\n",
        "\n",
        "df_paises = df_paises.groupBy(col('paises_exp')).mean().select('paises_exp', 'avg(duracion)').orderBy('paises_exp').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofjzZH2DEH8H"
      },
      "source": [
        "### 6. Filtra las películas que contengan la palabra music en su descripción y que su duración sea mayor a 90 minutos, ¿cuál es el actor que más películas ha realizado bajo estas condiciones?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_MboV7dJlF7"
      },
      "source": [
        "Para realizar esta parte, debo filtrar el dataframe por sinopsis que contenga la palabra \"music\", filtrar por las que la duración sea mayor de 90, y separar el elenco, ya que son arrays. Después, agrupo por elenco y hago un conteo para saber cuántas veces sale ese actor o actriz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8fCFyUkJmPI",
        "outputId": "ad87ecc9-afa7-41b1-dd4d-84ddc9eb7ec6"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import contains, count\n",
        "\n",
        "df_music = (\n",
        "    df.filter(col('sinopsis').contains('music'))\n",
        "    .filter('duracion > 90')\n",
        "    .withColumn('elenco', explode(col('elenco')))\n",
        ")\n",
        "\n",
        "actor_music = df_music.groupBy(col('elenco')).count().orderBy('count', ascending=False).first()[0]\n",
        "cantidad_music = df_music.groupBy(col('elenco')).count().orderBy('count', ascending=False).first()[1]\n",
        "\n",
        "print(f'El/la actor/actriz que más ha trabajado bajo estas condiciones es {actor_music} con {cantidad_music} películas.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qQSM6GwYl2f"
      },
      "source": [
        "### 7. Para el actor que más producciones ha realizado calcula cuántas semanas han pasado desde su primera producción hasta su última."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tjIaIgxYqT-"
      },
      "source": [
        "Primero, extraigo los actores del elenco para que estén cada uno en una fila con su producción (`df_producciones`). Después, agrupo por elenco y hago el conteo para sacar el que más producciones ha realizado (`actor_producciones`).\n",
        "\n",
        "Con este dato, hago un filtro en el que selecciono solo las producciones de ese actor (`primer_actor`).\n",
        "\n",
        "Las fechas de estreno solo están por año, por lo que paso los años a fecha (se considera que es el 1 de enero para cada uno) y de ahí obtengo la fecha menor y la mayor. Con `.days` obtengo el número de días, y al dividirlo entre 7, el número exacto de semanas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATzdV0KaYqn8",
        "outputId": "9b2c101f-d0fb-4af6-9a82-2f38b6eb7c9b"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import min, max\n",
        "\n",
        "df_producciones = df.withColumn('elenco', explode(col('elenco')))\n",
        "\n",
        "primer_actor = df_producciones.groupBy(col('elenco')).count().orderBy('count', ascending=False).first()[0] # Nombre de actor/actriz con más producciones.\n",
        "\n",
        "producciones_actor = df_producciones.filter(f'elenco == \"{primer_actor}\"').withColumn('estreno', to_date('estreno')) # Lista de producciones de ese actor/actriz.\n",
        "\n",
        "primera_prod = producciones_actor.select(min('estreno')).collect()[0][0] # Fecha de primera producción.\n",
        "ultima_prod = producciones_actor.select(max('estreno')).collect()[0][0] # Fecha de última producción.\n",
        "\n",
        "semanas = round((ultima_prod - primera_prod).days / 7, 2)\n",
        "\n",
        "print(f'El/la actor/actriz con más producciones es {primer_actor}, y han pasado {semanas} semanas desde su primera producción hasta la última.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oumyi2eI8SrF"
      },
      "source": [
        "### 8. Transforma la columna de géneros para que su contenido sea un array con los valores de cada género por registro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YdAUmLx8gMN"
      },
      "source": [
        "Esto ya lo realicé anteriormente en el apartado 4 al ajustar los tipos de cada columna.\n",
        "\n",
        "Individualmente sería con: `df = df.withColumn('genero', split(df['genero'], ', '))`, lo cual divide los géneros en base al separador de la coma y el espacio; y los añade a un array, de forma que cada registro tiene un array con los géneros.\n",
        "\n",
        "Adjunto el esquema para comprobar que está realizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USakRemH83w2",
        "outputId": "b3665d93-b0ae-43c5-dff2-9550ec27923b"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLrXfXCU9YkB"
      },
      "source": [
        "### 9. ¿Cuántas producciones se han realizado en un único país y cuántas tienen 2 o más países?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvZyBz1j9e_w"
      },
      "source": [
        "Para obtener las producciones por países, como es una columna con arrays, solamente tengo que filtrar por la columna, definiendo los que cuyo tamaño sea de 1 (es decir, un país) y los que cuyo tamaño sea de 2 o más (es decir, dos países o más).\n",
        "\n",
        "Hago un conteo con `count()` para obtener el número total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeFwvfZ59eeR",
        "outputId": "3663e712-41da-450f-8541-0fda49826b8e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import size\n",
        "\n",
        "prod_un_pais = df.filter(size(col('pais')) == 1).count()\n",
        "prod_varios_paises = df.filter(size(col('pais')) >= 2).count()\n",
        "\n",
        "print(f'''Hay {prod_un_pais} producciones realizadas por un único país.\n",
        "Hay {prod_varios_paises} producciones realizadas por dos o más países.''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVUiq1uqBif5"
      },
      "source": [
        "### 10. Escribe el dataframe final como un fichero parquet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_aULJk3Cux8"
      },
      "source": [
        "Para pasar el dataframe (`df`) a un archivo parquet, uso la función `write.parquet()`. Apunto la ruta y su nombre y elijo el modo sobreescribir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_jY6IcDBoiD"
      },
      "outputs": [],
      "source": [
        "df.write.parquet(path='/tmp/dataframe_netflix.parquet', mode='overwrite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRNCPquwC97r"
      },
      "source": [
        "Para comprobar que se guardó correctamente, pruebo a leer el archivo y mostrarlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cndRHeDjCXvB",
        "outputId": "072bfcf2-5d8b-4cef-e0d0-3e8c10ad8688"
      },
      "outputs": [],
      "source": [
        "prueba_parquet = spark.read.parquet('/tmp/dataframe_netflix.parquet')\n",
        "\n",
        "prueba_parquet.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "M6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
